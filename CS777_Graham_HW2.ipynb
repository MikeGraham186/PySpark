{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba19b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr, when,\\\n",
    "desc, first, last, row_number, rank, substring\n",
    "from pyspark.sql.functions import max as pmax, min as pmin\n",
    "import pyspark as ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70f8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f25efb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 675159 / 675159"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stocks_2013_2021 (3).txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = 'https://people.bu.edu/kalathur/datasets/stocks_2013_2021.txt'\n",
    "wget.download(fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97c6828",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17736/664383918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocksDF = spark.read.csv(\n",
    " \"stocks_2013_2021.txt\",\n",
    " header=False, inferSchema=True,\n",
    ").toDF('Stock','Date','Open','High',\n",
    " 'Low','Close','Volume'\n",
    " )\n",
    "stocksDF.printSchema()\n",
    "stocksDF.createOrReplaceTempView('stocks_table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1A\n",
    "stocksDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1B \n",
    "stocksDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e660729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1C\n",
    "stocksDF.groupBy(\"Stock\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1A\n",
    "stocksSQL = spark.sql(\"SELECT * FROM STOCKS_TABLE LIMIT 2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "spark.sql(\"SELECT COUNT(*) FROM STOCKS_TABLE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1C\n",
    "spark.sql(\"SELECT STOCK, COUNT(*) FROM STOCKS_TABLE GROUP BY STOCK\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c16fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2A\n",
    "stocksDF.select(['Stock', 'Volume']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2B\n",
    "stockVolume = stocksDF.select(['Stock', 'Volume']).groupBy(\"Stock\").sum(\"Volume\").\\\n",
    "withColumnRenamed(\"sum(Volume)\", \"Total_Volume\")\n",
    "stockVolume.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2C\n",
    "stockVolume.sort(\"Total_Volume\", ascending = False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd847a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2A\n",
    "spark.sql(\"SELECT STOCK, VOLUME FROM STOCKS_TABLE LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc65d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2B\n",
    "spark.sql(\"SELECT STOCK, SUM(VOLUME) TOTAL_VOLUME FROM STOCKS_TABLE GROUP BY STOCK\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2C\n",
    "spark.sql(\"SELECT STOCK, SUM(VOLUME) TOTAL_VOLUME FROM STOCKS_TABLE GROUP BY STOCK ORDER BY TOTAL_VOLUME DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd7cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3A\n",
    "stockOpenClose = stocksDF.select([\"Stock\", \"Open\", \"Close\"])\n",
    "stockOpenClose.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3B\n",
    "stockOpenClose.filter(stockOpenClose.Close > stockOpenClose.Open).groupBy(\"Stock\").count().\\\n",
    "withColumnRenamed(\"count\",\"Count\")\\\n",
    ".sort(\"Count\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde766aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f65297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3A\n",
    "spark.sql(\"SELECT STOCK, OPEN, CLOSE FROM STOCKS_TABLE LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3B\n",
    "spark.sql(\"SELECT STOCK, SUM(CASE WHEN S.CLOSE > S.OPEN THEN 1 ELSE 0 END) TALLY FROM STOCKS_TABLE S GROUP BY STOCK \\\n",
    "          ORDER BY TALLY DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be6817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c663166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4A\n",
    "highLow = stocksDF.select([\"Stock\", \"High\", \"Low\"])\n",
    "highLow.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4B\n",
    "highLow.groupBy(\"Stock\").agg({\"High\": \"max\", \"Low\": \"min\"})\\\n",
    ".withColumnRenamed(\"max(High)\", \"Max_High\").withColumnRenamed(\"min(low)\", \"Min_Low\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 - SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec84b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4A\n",
    "spark.sql(\"SELECT STOCK, HIGH, LOW FROM STOCKS_TABLE LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29846f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4B\n",
    "spark.sql(\"SELECT STOCK, MAX(HIGH) MAX_HIGH, MIN(LOW) MAX_LOW FROM STOCKS_TABLE GROUP BY STOCK\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6883c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe12773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5A\n",
    "highLow = stocksDF.select(col(\"Stock\"), col(\"High\"), col(\"Date\").alias(\"HighDate\"), \\\n",
    "                          col(\"Low\"), col(\"Date\").alias(\"LowDate\"))\n",
    "highLow.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833caaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a218bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5A\n",
    "spark.sql(\"SELECT STOCK, HIGH, DATE HIGHDATE, LOW, DATE LOWDATE FROM STOCKS_TABLE LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 5B\n",
    "spark.sql(\"SELECT STOCK, HIGH, DATE FROM STOCKS_TABLE A JOIN \\\n",
    "        (SELECT STOCK TEMPTICK, MAX(HIGH) MAXHIGH FROM STOCKS_TABLE GROUP BY STOCK) B ON \\\n",
    "        A.STOCK = B.TEMPTICK AND A.HIGH = B.MAXHIGH \\\n",
    "        UNION \\\n",
    "          SELECT STOCK, LOW, DATE FROM STOCKS_TABLE A JOIN \\\n",
    "        (SELECT STOCK TEMPTICK, MIN(LOW) MINLOW FROM STOCKS_TABLE GROUP BY STOCK) B ON \\\n",
    "        A.STOCK = B.TEMPTICK AND A.LOW = B.MINLOW ORDER BY STOCK\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70414ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockYearVolume = stocksDF.select([\"Stock\", \"Date\", \"Volume\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fee808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6A\n",
    "stockYearVolume = stocksDF.select([\"Stock\", \"Date\", \"Volume\"])\n",
    "stockYearVolume = stockYearVolume.select(col(\"Stock\"), substring(col(\"Date\"), 0, 4), col(\"Volume\"))\\\n",
    ".withColumnRenamed(\"substring(Date, 0, 4)\", \"Year\")\n",
    "stockYearVolume.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff143cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6A\n",
    "spark.sql(\"SELECT STOCK, LEFT(DATE, 4) YEAR, VOLUME FROM STOCKS_TABLE\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 6B\n",
    "spark.sql(\"SELECT STOCK, YEAR, SUM(VOLUME) FROM (SELECT STOCK, LEFT(DATE, 4) YEAR, VOLUME FROM STOCKS_TABLE) B \\\n",
    "GROUP BY STOCK, YEAR ORDER BY STOCK, YEAR\").show()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ecc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 7 - Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e545420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a9218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf63c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 7 - SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 7A\n",
    "spark.sql(\"SELECT STOCK, LEFT(DATE, 4) YEAR, HIGH, LOW FROM STOCKS_TABLE\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 7B\n",
    "spark.sql(\"SELECT STOCK, LEFT(DATE, 4) YEAR, MAX(HIGH) MAX_HIGH, MIN(LOW) MIN_LOW FROM STOCKS_TABLE \\\n",
    "GROUP BY STOCK, YEAR ORDER BY STOCK, YEAR\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd05d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 7C\n",
    "spark.sql(\"SELECT A.STOCK, B.YEAR, A.HIGH, A.DATE, A.LOW, A.DATE FROM STOCKS_TABLE A JOIN \\\n",
    "          (SELECT STOCK TEMPTICKER, LEFT(DATE, 4) YEAR, MAX(HIGH) MAX_HIGH, MIN(LOW) MIN_LOW FROM STOCKS_TABLE \\\n",
    "GROUP BY STOCK, YEAR) B \\\n",
    "ON A.STOCK = B.TEMPTICKER AND \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
